{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dab58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "#from matplotlib import rc\n",
    "import numpy as np\n",
    "\n",
    "from two_d_ground_state import *\n",
    "import matrix_element as me\n",
    "\n",
    "from os import path\n",
    "import os\n",
    "from copy import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da216bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = False\n",
    "\n",
    "# Check to see if gpu is available. If it is, use it else use the cpu\n",
    "if torch.cuda.is_available() and use_gpu:\n",
    "    device = torch.device('cuda')\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "    print('Using GPU.')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    torch.set_default_tensor_type('torch.DoubleTensor')\n",
    "\n",
    "    if not torch.cuda.is_available() and use_gpu: \n",
    "        use_gpu = False \n",
    "        print('GPU not available. Using CPU.')\n",
    "    else: \n",
    "        print('Using CPU.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a03b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_list = [3]\n",
    "N = N_list[0]\n",
    "model_list = []\n",
    "\n",
    "M = 16\n",
    "#w = M\n",
    "w = 4\n",
    "#h = int(M/w)\n",
    "h = 4\n",
    "pbc = False\n",
    "\n",
    "for (n_idx, N) in enumerate(N_list): \n",
    "    O = N\n",
    "    \n",
    "    model = me.Bose_Hubbard(N, O, w=w, h=h, M=M, pbc=pbc)\n",
    "    model_list.append(model)\n",
    "    \n",
    "print(\"The size of the Hamiltonian is {}\".format(model.tot_states))\n",
    "\n",
    "t = 1 \n",
    "U_max = 5\n",
    "V = 0\n",
    "U_list_all = np.arange(1, U_max+1, .25)\n",
    "#U_list_all =np.array([4])\n",
    "mu_list_all = np.linspace(0, 10, 5)\n",
    "mu_list_all = np.array([0.])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cbba22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "## NN\n",
    "\n",
    "S = 50 # number of sample of the MH sampler (not used)\n",
    "init = 1 # the first state to sample in Metropolis Hastings (has nothing to do with the optimizer!) (not used)\n",
    "\n",
    "# Network parameters\n",
    "D_hid = 400 # the number ofneurons in the hidden layer\n",
    "lr = 5e-6 # learning rate \n",
    "epochs = 70000\n",
    "loss_diff = 1e-7\n",
    "grad_cut = 1e-6\n",
    "check_point = 100 # print out the energy every X points\n",
    "use_sampler = False # for now, only support ground state (not working anyway)\n",
    "\n",
    "# Model parameters\n",
    "U_train = np.ones(3)*2\n",
    "t_train = 1.\n",
    "\n",
    "\n",
    "U_train = np.logspace(-2, 2, 9, endpoint = True)\n",
    "mu_train = np.zeros_like(U_train)\n",
    "\n",
    "min_state = 0\n",
    "max_state = 1\n",
    "\n",
    "n_excited = max_state - 1\n",
    "\n",
    "# paths to save and load weights \n",
    "#fpath ='/storage/disqs/phrczh/HubbardNet/weights'\n",
    "\n",
    "#t0 = time.time()\n",
    "\n",
    "loss_all = [] \n",
    "penalty_all = []\n",
    "nn = []\n",
    "\n",
    "# filepath for excited states\n",
    "fname = \"./weights/2D-ADAM-weights_M{}_N{}_w{}_h{}_V{}_D{}lr{}U_train{}\".format(M,N,w,h,V,D_hid,lr,U_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6b2f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "def call_NN(lr, n_excited):\n",
    "    if n_excited: \n",
    "        if n_excited == 1 :\n",
    "            load_states_indv = [0]\n",
    "        else:\n",
    "            load_states_indv = range(1, n_excited)\n",
    "        gs_flag = False\n",
    "        es_flag = True\n",
    "\n",
    "    else: # ground states\n",
    "        load_states_indv = [0]\n",
    "        gs_flag = True\n",
    "        es_flag = False\n",
    "    \n",
    "    load_states = np.max(load_states_indv)  # total number of states being fixed \n",
    "  \n",
    "    params = {'D_hid': D_hid, \n",
    "              'step_size': lr, \n",
    "              'max_iteration':epochs,\n",
    "              'check_point': check_point,\n",
    "              'loss_diff': loss_diff, \n",
    "              'steps': 1000, # reset learning every N steps\n",
    "              'loss_check_steps': 50, # check the local every N steps\n",
    "              'grad_cut': grad_cut,  # stopping condition in the total gradient \n",
    "              'weight_init': False, \n",
    "              'zero_bias': False, \n",
    "              'gs_epochs': 1000, # the maximum number of steps to minimize the ground state\n",
    "              'gs_flag': gs_flag, # ground state only\n",
    "              'es_flag': es_flag,  # excited state only\n",
    "              'regularization': True, \n",
    "              'load_states': load_states, # the number of states loaded \n",
    "              'load_states_indv': load_states_indv, \n",
    "              'rand_steps': 5000, \n",
    "              'load_weights_from_previous_state': False, # randomize the projection every N steps\n",
    "              'use_gpu': use_gpu, \n",
    "              'weight_decay': 0,\n",
    "              'perturb_amp': 0.00, \n",
    "              'dropout': 0.0}\n",
    "  \n",
    "    print(\"Begin optimizing for state {}\".format(n_excited))\n",
    "\n",
    "    fc1, Loss_history, dot_history, all_E_list = train_NN(model_list, N_list, mu_train, U_train, t_train, V, S, params, fname, \\\n",
    "                                            use_sampler=use_sampler, init=init, loadweights=False,\\\n",
    "                                            fname_load=fname, n_excited=n_excited)\n",
    "\n",
    "    return fc1, Loss_history, dot_history\n",
    "\n",
    "\n",
    "\n",
    "for n_excited in range(min_state, max_state):\n",
    "    \n",
    "    fc1, Loss_history, dot_history = call_NN(lr, n_excited)\n",
    "\n",
    "    nn.append(fc1)\n",
    "    loss_all.append(Loss_history)\n",
    "    penalty_all.append(dot_history)\n",
    "    \n",
    "tf = time.time()\n",
    "print(\"Training time = {} seconds.\".format(tf-t0))\n",
    "#np.savetxt(f'./time/M-{M}-N-{N}-U_train-{U_train}-lr-{lr}-num_th-{torch.get_num_threads()}-interop-{torch.get_num_interop_threads()}-use_gpu-{use_gpu}-adam.csv',[tf-t0])\n",
    "#np.savetxt(f'./time/steps-1000-epochs-{epochs}-ADAM-M-{M}-N-{N}-U_train-{U_train}-lr-{lr}-num_th-{torch.get_num_threads()}-interop-{torch.get_num_interop_threads()}-use_gpu-{use_gpu}.csv',[tf-t0])\n",
    "#np.savetxt(f'./time/loss_check_steps-10-ADAM-M-{M}-N-{N}-U_train-{U_train}-lr-{lr}-num_th-{torch.get_num_threads()}-interop-{torch.get_num_interop_threads()}-use_gpu-{use_gpu}.csv',[tf-t0])\n",
    "np.savetxt(f'./time/2D-ADAM-M-{M}-N-{N}-w{w}-h{h}-len_U_train-{len(U_train)}-lr-{lr}-num_th-{torch.get_num_threads()}-interop-{torch.get_num_interop_threads()}-use_gpu-{use_gpu}.csv',[tf-t0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d734eb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "U_test = np.logspace(-2.125, 2.125, 69, endpoint = True)  \n",
    "\n",
    "mu_test = np.zeros_like(U_test)\n",
    "\n",
    "\n",
    "E_train, wf_gs = wf_e_calc(model_list[0], N_list[0], U_train, mu_train, t, V, 0, 0, fc1, use_gpu=False)\n",
    "E_test, wf_gs_test = wf_e_calc(model_list[0], N_list[0], U_test, mu_test, t, V, 0, 0, fc1, use_gpu=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
