{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ptXLeykcsTVo"
   },
   "source": [
    "# Example calculation of the ground state of Bose-Hubbard model for multiple N's to get the phase diagram\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2QYkEBxmbZJr",
    "outputId": "c872109b-04cf-44c3-f066-a595b3290ba4"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ptIfFLcHbfxT",
    "outputId": "837d44cc-2f2a-49e9-efb1-3302ff859bb2"
   },
   "outputs": [],
   "source": [
    "# # including necessary files and latex fonts\n",
    "# !cp drive/MyDrive/DNN/HubbardNet_gpu.py .\n",
    "# !cp drive/MyDrive/DNN/matrix_element.py .\n",
    "# !sudo apt install cm-super dvipng texlive-latex-extra texlive-latex-recommended\n",
    "# !pip install matplotlib==3.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Lfug1k7VbgUv"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dw/00xfphjj7mqfdx9qd2n3glth0000gp/T/ipykernel_32994/1424705224.py:23: MatplotlibDeprecationWarning: Support for setting an rcParam that expects a str value to a non-str value is deprecated since 3.5 and support will be removed two minor releases later.\n",
      "  matplotlib.rcParams['text.latex.preamble']=[r'\\usepackage{amsmath}']\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from matplotlib import rc\n",
    "import numpy as np\n",
    "\n",
    "from HubbardNet_gpu import *\n",
    "import matrix_element as me\n",
    "\n",
    "from os import path\n",
    "import os\n",
    "from copy import copy\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "plt.rc('text',usetex=True)\n",
    "#font = {'family':'serif','size':16}\n",
    "font = {'family':'serif','size':25, 'serif': ['computer modern roman']}\n",
    "plt.rc('font',**font)\n",
    "matplotlib.rcParams['text.latex.preamble']=[r'\\usepackage{amsmath}']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GKDQu5TPcR-p",
    "outputId": "02222d89-b3ec-404e-ae92-fba3a23d919e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU.\n"
     ]
    }
   ],
   "source": [
    "# Go to Edit -> Notebook Settings and select \"GPU\" from the hardware accelerator dropdown. \n",
    "# If this is on, GPU is enabled by default\n",
    "\n",
    "use_gpu = False\n",
    "\n",
    "# Check to see if gpu is available. If it is, use it else use the cpu\n",
    "if torch.cuda.is_available() and use_gpu:\n",
    "    device = torch.device('cuda')\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "    print('Using GPU.')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    torch.set_default_tensor_type('torch.DoubleTensor')\n",
    "\n",
    "    if not torch.cuda.is_available() and use_gpu: \n",
    "        use_gpu = False \n",
    "        print('GPU not available. Using CPU.')\n",
    "    else: \n",
    "        print('Using CPU.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # charge gap & chemical potential (ED)\n",
    "# M_list = np.arange(3, 6)\n",
    "# U_list = np.linspace(2, 100, 20)\n",
    "# t = 1\n",
    "# V = 0\n",
    "# mu = 0\n",
    "# pbc = True\n",
    "\n",
    "# E_gs = np.zeros( (len(N_list), len(M_list), len(U_list)) )\n",
    "# for (m_idx, M) in enumerate(M_list):\n",
    "    \n",
    "#     N_list = [M-1, M, M+1]\n",
    "#     model_list = []\n",
    "#     H_list = []\n",
    "\n",
    "#     w = M\n",
    "#     h = int(M/w)\n",
    "    \n",
    "\n",
    "#     for (n_idx, N_here) in enumerate(N_list): \n",
    "#         O = N_here # maximum number of particles \n",
    "#         model = me.Bose_Hubbard(N_here, O, w=w, h=h, M=M, pbc=pbc)\n",
    "#         for (U_idx, U) in enumerate(U_list):\n",
    "#             _,_,H = model.H_Bose_Hubbard(t, U, V=V, mu=mu)\n",
    "#             vals, vecs = np.linalg.eig( H )\n",
    "            \n",
    "#             E_gs[n_idx, m_idx, U_idx] = np.min(vals.real)\n",
    "            \n",
    "#     print(\"Done with lattice size {}/{}\".format(m_idx+1, len(M_list)))\n",
    "\n",
    "\n",
    "# # charge gap \n",
    "# cgap = E_gs[0] + E_gs[2] - 2*E_gs[1]\n",
    "# mu_plus = E_gs[2] - E_gs[1]\n",
    "# mu_minus = E_gs[1] - E_gs[0]\n",
    "\n",
    "    \n",
    "# # print(\"The size of the Hamiltonian is {}\".format(model.tot_states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig,ax=plt.subplots(figsize=(7,5))\n",
    "# for (m_idx, M) in enumerate(M_list):\n",
    "#     color = next(ax._get_lines.prop_cycler)['color']\n",
    "#     ax.plot(t/U_list, mu_plus[m_idx]/U_list, '-', color=color, label='$M = ' +str(M) +\"$\")\n",
    "#     ax.plot(t/U_list, mu_minus[m_idx]/U_list, '-', color=color)\n",
    "#     ax.set_xlabel('$t/U$')\n",
    "#     ax.set_ylabel('$\\mu^\\pm/U$')\n",
    "#     ax.legend(fontsize=15)\n",
    "#     ax.set_ylim([-0.23, 1])\n",
    "#     ax.set_xlim([0, 0.5])\n",
    "# plt.show()\n",
    "# # plt.savefig('./figures/ED_chemical_potential.pdf', format='pdf',bbox_inches='tight')\n",
    "\n",
    "# # plt.figure()\n",
    "# # plt.plot(t/U_list,cgap/U_list, '-s')\n",
    "# # plt.xlabel('$t/U$')\n",
    "# # plt.ylabel('Charge gap $(t)$')\n",
    "# # plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the Hamiltonian is 924\n",
      "The size of the Hamiltonian is 1716\n",
      "The size of the Hamiltonian is 3003\n"
     ]
    }
   ],
   "source": [
    "model_list = []\n",
    "\n",
    "M = 7\n",
    "N_list = [M-1,M,M+1]\n",
    "\n",
    "w = M\n",
    "h = int(M/w)\n",
    "pbc = True\n",
    "\n",
    "for (n_idx, N) in enumerate(N_list): \n",
    "    O = N\n",
    "    \n",
    "    model = me.Bose_Hubbard(N, O, w=w, h=h, M=M, pbc=pbc)\n",
    "    model_list.append(model)\n",
    "    \n",
    "    print(\"The size of the Hamiltonian is {}\".format(model.tot_states))\n",
    "\n",
    "t = 1 \n",
    "U_max = 5\n",
    "V = 0\n",
    "U_list_all = np.arange(1, U_max+1, .5)\n",
    "mu_list_all = np.linspace(0, 10, 5)\n",
    "mu_list_all = np.array([0.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "560604jwhqey"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "## NN\n",
    "\n",
    "S = 50 # number of sample of the MH sampler (not used)\n",
    "init = 1 # the first state to sample in Metropolis Hastings (has nothing to do with the optimizer!) (not used)\n",
    "\n",
    "# Network parameters\n",
    "D_hid = 400 # the number ofneurons in the hidden layer\n",
    "lr = 0.01 # learning rate \n",
    "epochs = 70000\n",
    "loss_diff = 1e-7\n",
    "grad_cut = 1e-6\n",
    "check_point = 100 # print out the energy every X points\n",
    "use_sampler = False # for now, only support ground state (not working anyway)\n",
    "\n",
    "# Model parameters\n",
    "U_train = np.ones(3)*2\n",
    "t_train = 1.\n",
    "\n",
    "U_train = np.array([4., 7, 10])\n",
    "mu_train = np.zeros_like(U_train)\n",
    "\n",
    "min_state = 0\n",
    "max_state = 1\n",
    "\n",
    "n_excited = max_state - 1\n",
    "\n",
    "# paths to save and load weights \n",
    "fpath = os.getcwd()+'/weights/'\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "loss_all = [] \n",
    "penalty_all = []\n",
    "nn = []\n",
    "\n",
    "# filepath for excited states\n",
    "fname = fpath + \"/weights_multi_N_M{}_Umax{}_Umin{}\".format(M,np.max(U_train),np.min(U_train),D_hid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "97XH_A3ol_l_",
    "outputId": "df75b1ac-613f-4ae8-8318-744e3606c5e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin optimizing for state 0\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zoe/Desktop/CMT/DNN/HubbardNet/HubbardNet_gpu.py:727: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at  ../aten/src/ATen/native/Copy.cpp:240.)\n",
      "  H_list.append(torch.tensor(H, dtype=torch.double))\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:508: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, current learning rate 0.01, loss 24.47518865483529, total gradient 4.9697569600992715\n",
      "Iteration 100, current learning rate 0.00975045441899919, loss 20.601238279474092, total gradient 3.2018436959501964\n",
      "Iteration 200, current learning rate 0.009035881413933598, loss 24.51689265087994, total gradient -0.028023894694732855\n",
      "Iteration 300, current learning rate 0.007926253039256266, loss 24.51231259504044, total gradient -0.005693154558566148\n",
      "Iteration 400, current learning rate 0.0065301874465082295, loss 21.46714605036063, total gradient 2.945790288948739\n",
      "Iteration 500, current learning rate 0.004984341255885152, loss 21.2888118534903, total gradient 54.903933312726295\n",
      "Iteration 600, current learning rate 0.0034400326517927448, loss 21.535874240762524, total gradient -12.146865200202802\n",
      "Iteration 700, current learning rate 0.002048429298162417, loss 22.111266118865753, total gradient -0.6007079041860954\n",
      "Iteration 800, current learning rate 0.0009457509696090281, loss 22.04297854677185, total gradient -0.060600245323464225\n",
      "Iteration 900, current learning rate 0.00023993524647491373, loss 22.035563392390593, total gradient -0.049315645860331825\n",
      "Reset scheduler\n",
      "Iteration 1000, current learning rate 0.01, loss 22.034869308444563, total gradient -0.04885015417538179\n",
      "Iteration 1100, current learning rate 0.00975045441899919, loss 24.514026152428663, total gradient -0.023587206408173095\n",
      "Iteration 1200, current learning rate 0.009035881413933598, loss 23.64718924478457, total gradient -3.071987835373623\n",
      "Iteration 1300, current learning rate 0.007926253039256266, loss 23.137761098519793, total gradient 6.390822213515373\n",
      "Iteration 1400, current learning rate 0.0065301874465082295, loss 24.513590606042516, total gradient -0.007615787516193335\n",
      "Iteration 1500, current learning rate 0.004984341255885152, loss 24.519076683110818, total gradient 0.004015792611263428\n",
      "Iteration 1600, current learning rate 0.0034400326517927448, loss 22.447979234775037, total gradient 2.002422406125764\n",
      "Iteration 1700, current learning rate 0.002048429298162417, loss 23.665281889768412, total gradient 2.409243485680633\n"
     ]
    }
   ],
   "source": [
    "def call_NN(lr, n_excited):\n",
    "    if n_excited: \n",
    "        if n_excited == 1 :\n",
    "            load_states_indv = [0]\n",
    "        else:\n",
    "            load_states_indv = range(1, n_excited)\n",
    "        gs_flag = False\n",
    "        es_flag = True\n",
    "\n",
    "    else: # ground states\n",
    "        load_states_indv = [0]\n",
    "        gs_flag = True\n",
    "        es_flag = False\n",
    "    \n",
    "    load_states = np.max(load_states_indv)  # total number of states being fixed \n",
    "  \n",
    "    params = {'D_hid': D_hid, \n",
    "              'step_size': lr, \n",
    "              'max_iteration':epochs,\n",
    "              'check_point': check_point,\n",
    "              'loss_diff': loss_diff, \n",
    "              'steps': 1, # reset learning every N steps\n",
    "              'loss_check_steps': 50, # check the local every N steps\n",
    "              'grad_cut': grad_cut,  # stopping condition in the total gradient \n",
    "              'weight_init': False, \n",
    "              'zero_bias': False, \n",
    "              'gs_epochs': 1000, # the maximum number of steps to minimize the ground state\n",
    "              'gs_flag': gs_flag, # ground state only\n",
    "              'es_flag': es_flag,  # excited state only\n",
    "              'regularization': True, \n",
    "              'load_states': load_states, # the number of states loaded \n",
    "              'load_states_indv': load_states_indv, \n",
    "              'rand_steps': 5000, \n",
    "              'load_weights_from_previous_state': False, # randomize the projection every N steps\n",
    "              'use_gpu': use_gpu, \n",
    "              'weight_decay': 0.01,\n",
    "              'perturb_amp': 0.0, \n",
    "              'dropout': 0.0}\n",
    "  \n",
    "    print(\"Begin optimizing for state {}\".format(n_excited))\n",
    "\n",
    "    fc1, Loss_history, dot_history, all_E_list = train_NN(model_list, N_list, mu_train, U_train, t_train, V, S, params, fname, \\\n",
    "                                            use_sampler=use_sampler, init=init, loadweights=False,\\\n",
    "                                            fname_load=fname, n_excited=n_excited)\n",
    "\n",
    "    return fc1, Loss_history, dot_history\n",
    "\n",
    "\n",
    "\n",
    "for n_excited in range(min_state, max_state):\n",
    "    \n",
    "    fc1, Loss_history, dot_history = call_NN(lr, n_excited)\n",
    "    print(len(Loss_history))\n",
    "\n",
    "    nn.append(fc1)\n",
    "    loss_all.append(Loss_history)\n",
    "    penalty_all.append(dot_history)\n",
    "    \n",
    "tf = time.time()\n",
    "print(\"Training time = {} seconds.\".format(tf-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 358
    },
    "id": "mi1QNIyMmee4",
    "outputId": "12983a86-a2a7-4a21-931a-442ee39889c6"
   },
   "outputs": [],
   "source": [
    "\n",
    "for i in range(1): \n",
    "    Loss_history = loss_all[i]\n",
    "    Loss_history = np.array(Loss_history)\n",
    "    \n",
    "    fig,ax=plt.subplots(figsize=(7,5))\n",
    "    ax.plot(Loss_history - np.min(Loss_history), '-', label='loss')\n",
    "    ax.set_xlabel(\"Iteration\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    # ax.set_title(\"U/t = {}\".format(U_train))\n",
    "    ax.set_yscale('log')\n",
    "    \n",
    "    \n",
    "    # ax.plot(dot_history, '-', label='regularization')\n",
    "    ax.set_xlabel(\"Iteration\")\n",
    "    # ax.set_ylabel(\"Dot product\")\n",
    "    # ax.set_title(\"U/t = {}\".format(U_train))\n",
    "    # ax.legend()\n",
    "    plt.savefig('./figures/loss_M{}.pdf'.format(M,N,range(min_state, max_state)[i]), format='pdf',bbox_inches='tight')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JNSHzI8XmgID",
    "outputId": "9e1de8c0-9113-447d-a910-1979828d8280"
   },
   "outputs": [],
   "source": [
    "E_all = np.zeros(( len(U_train), model.tot_states) )\n",
    "# check solution \n",
    "for (i, U) in enumerate(U_train):\n",
    "    _, _, H = model.H_Bose_Hubbard(t, U, V=V, mu=mu_train[i])\n",
    "    evals, evecs = np.linalg.eig(H)\n",
    "    idx = np.argsort(evals)\n",
    "    evecs = evecs[:,idx]\n",
    "    evals = evals[idx]\n",
    "    E_all[i] = evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4WZpMkuW0ja0"
   },
   "outputs": [],
   "source": [
    "U_test = np.linspace(1.5, 15, 20)\n",
    "mu_test = np.zeros_like(U_test)\n",
    "\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "plt.rc('text',usetex=True)\n",
    "#font = {'family':'serif','size':16}\n",
    "font = {'family':'serif','size':25, 'serif': ['computer modern roman']}\n",
    "plt.rc('font',**font)\n",
    "matplotlib.rcParams['text.latex.preamble']=[r'\\usepackage{amsmath}']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7,5))\n",
    "\n",
    "colors = ['r','b', 'k']\n",
    "for j,model in enumerate(model_list):\n",
    "    E_train, wf_gs = wf_e_calc(model_list[j], N_list[j], U_train, mu_train, t, V, 0, 0, fc1, use_gpu=True)\n",
    "    E_test, wf_gs_test = wf_e_calc(model_list[j], N_list[j], U_test, mu_test, t, V, 0, 0, fc1, use_gpu=True)\n",
    "\n",
    "    \n",
    "    ax.scatter(U_train,E_train,s=50,c=colors[j], marker='s')\n",
    "    ax.scatter(U_test,E_test,s=50, c=colors[j], marker='x',lw=2)\n",
    "\n",
    "    arr = [] \n",
    "    for (i, U) in enumerate(U_test):\n",
    "        _, _, H = model.H_Bose_Hubbard(t, U, V=V, mu=mu_train[0])\n",
    "        vals, vecs = np.linalg.eig(H)\n",
    "        vals_idx = np.argsort(vals)\n",
    "        vals = np.sort(vals)\n",
    "        arr.append(vals[0])\n",
    "    ax.plot(U_test,arr,colors[j],label='N={}'.format(N_list[j]))\n",
    "ax.set_xlim([min(U_test), max(U_test)])  \n",
    "ax.set_ylabel('Energy')\n",
    "ax.set_xlabel('$U$')\n",
    "plt.legend(frameon=False,prop={'size': 16})\n",
    "plt.savefig(\"./figures/energy_M{}.pdf\".format(M), format='pdf',bbox_inches='tight')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "id": "szreTRyvwcZX",
    "outputId": "e291ca65-c99c-46fa-d735-99678c71cdaa"
   },
   "outputs": [],
   "source": [
    "# chemical potential & charge gap\n",
    "U_test = np.linspace(0.1, 20, 40)\n",
    "mu_test = np.zeros_like(U_test)\n",
    "E_gs_nn = np.zeros( (len(N_list), len(U_test)) )\n",
    "E_gs_train = np.zeros( (len(N_list), len(U_train)) )\n",
    "\n",
    "colors = ['r','b','k']\n",
    "for j,model in enumerate(model_list):\n",
    "    E_test, wf_gs_test = wf_e_calc(model_list[j], N_list[j], U_test, mu_test, t, V, 0, 0, fc1, use_gpu=True)\n",
    "    E_train, wf_gs = wf_e_calc(model_list[j], N_list[j], U_train, mu_train, t, V, 0, 0, fc1, use_gpu=True)\n",
    "    E_gs_nn[j,:] = E_test.T\n",
    "    E_gs_train[j,:] = E_train.T\n",
    "    \n",
    "cgap = E_gs_nn[0] + E_gs_nn[2] - 2*E_gs_nn[1]\n",
    "mu_plus = E_gs_nn[2] - E_gs_nn[1]\n",
    "mu_minus = E_gs_nn[1] - E_gs_nn[0]\n",
    "\n",
    "cgap_train = E_gs_train[0] + E_gs_train[2] - 2*E_gs_train[1]\n",
    "mu_plus_train = E_gs_train[2] - E_gs_train[1]\n",
    "mu_minus_train = E_gs_train[1] - E_gs_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "4USQHKkMjGVP",
    "outputId": "e1bfe25a-00f4-4529-8564-c4d81dea11dd"
   },
   "outputs": [],
   "source": [
    "fig, ax=plt.subplots(figsize=(7,5))\n",
    "color = next(ax._get_lines.prop_cycler)['color']\n",
    "ax.plot(t/U_test, mu_plus/U_test, 'x', color=color)\n",
    "ax.plot(t/U_test, mu_minus/U_test, 'x', color=color)\n",
    "ax.plot(t/U_train, mu_plus_train/U_train, 's', color=color)\n",
    "ax.plot(t/U_train, mu_minus_train/U_train, 's', color=color)\n",
    "\n",
    "ax.set_xlabel('$t/U$')\n",
    "ax.set_ylabel('$\\mu^\\pm/U$')\n",
    "ax.set_ylim([-0.23, 1])\n",
    "ax.set_xlim([0, 0.5])\n",
    "plt.show()\n",
    "plt.savefig('./figures/NN_chemical_potential_M{}.pdf'.format(M), format='pdf',bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "id": "DY5M3VF7s5fV",
    "outputId": "34d2142a-8753-4919-f1ce-7ee675604849"
   },
   "outputs": [],
   "source": [
    "n_list = np.zeros( (len(model_list),M) )\n",
    "\n",
    "fig, ax = plt.subplots(1, len(model_list), figsize=(15, 5))\n",
    "U_test = np.array( [5] ) \n",
    "\n",
    "for model_idx,model in enumerate(model_list):\n",
    "    # rearrange states by symmetry \n",
    "    all_states = np.zeros_like(model.all_states)\n",
    "    idx_list = np.zeros(model.tot_states,dtype=int)\n",
    "\n",
    "    for i in range(int(model.tot_states/2)): \n",
    "        all_states[i] = model.all_states[i]\n",
    "        all_states[-i-1] = np.flip(model.all_states[i])\n",
    "        idx_list[i] = i\n",
    "        for j in range(model.tot_states):\n",
    "            if all(model.all_states[j]==all_states[-i-1]): \n",
    "                idx_list[-i-1] = j\n",
    "                break\n",
    "\n",
    "    # check wavefunctions \n",
    "    \n",
    "    mu_test = np.zeros(len(U_test))\n",
    "    vals_all = np.zeros((model.tot_states, len(U_test)))\n",
    "    vals_idx_all = np.zeros_like(vals_all)\n",
    "    vecs_all = np.zeros((model.tot_states, model.tot_states, len(U_test)))\n",
    "\n",
    "    \n",
    "    _, _, H = model.H_Bose_Hubbard(t, U, mu=mu_train[0])\n",
    "    vals, vecs = np.linalg.eig(H)\n",
    "    vals_idx_all[:,0] = np.argsort(vals)\n",
    "    vals_all[:,0] = vals\n",
    "    vecs_all[:,:,0] = vecs\n",
    "\n",
    "    E_test, wf_test = wf_e_calc(model, N_list[model_idx], U_test, mu_test, t, V, 0, 0, fc1, use_gpu=True)\n",
    "    \n",
    "    s = 0 \n",
    "    if model_idx == 0:\n",
    "        ymax = 0.4\n",
    "    else: \n",
    "        ymax = 0.3\n",
    "    ymin = 0\n",
    "  \n",
    "    i=0\n",
    "    wf_here = wf_test[i*model.tot_states:(i+1)*model.tot_states,s].squeeze().double()\n",
    "\n",
    "    # calculate occupation numbers\n",
    "    for m_idx in range(M):\n",
    "        for state_idx in range(model.tot_states):\n",
    "            ci = wf_here[state_idx].cpu().detach().numpy()\n",
    "            ni = model.all_states[state_idx][m_idx]\n",
    "            n_list[model_idx,m_idx] += ci**2 * ni\n",
    "\n",
    "    wf_exact = np.abs(vecs_all[:,int(vals_idx_all[s, i]), i]).squeeze()\n",
    "    wf_nn = np.abs(wf_here.cpu().detach().numpy())\n",
    "    ax[model_idx].plot(wf_exact[idx_list], label=\"ED\")\n",
    "    ax[model_idx].plot(wf_nn[idx_list], 'x--', label=\"HubbardNet\")\n",
    "    \n",
    "    ax[model_idx].set_title(\"N = {}\".format(N_list[model_idx]))\n",
    "    ax[model_idx].set_ylim((ymin,ymax))\n",
    "\n",
    "ax[0].set_ylabel('$|\\Psi_0 (\\mathbf{n})|$')\n",
    "ax[0].set_xlabel('Component')\n",
    "ax[1].set_xlabel('Component')\n",
    "ax[1].legend(frameon=True, prop={'size': 16})\n",
    "plt.savefig(os.getcwd() + \"/figures/wf_multi_N_M{}_gs.pdf\".format(M,N_list[model_idx],s,n_excited), format='pdf',bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gUVsCG8OKx0X"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(wf_nn[idx_list])\n",
    "# plt.plot(wf_exact[idx_list])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(wf_exact[idx_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f1XVgxi7KyUv"
   },
   "outputs": [],
   "source": [
    "np.sum(wf_exact[idx_list]**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_list_srt = np.sort(idx_list)\n",
    "plt.figure()\n",
    "plt.plot(idx_list_srt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
