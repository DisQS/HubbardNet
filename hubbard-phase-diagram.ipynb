{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ptXLeykcsTVo"
   },
   "source": [
    "# Example calculation of the ground state of Bose-Hubbard model for multiple N's to get the phase diagram\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2QYkEBxmbZJr",
    "outputId": "c872109b-04cf-44c3-f066-a595b3290ba4"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ptIfFLcHbfxT",
    "outputId": "837d44cc-2f2a-49e9-efb1-3302ff859bb2"
   },
   "outputs": [],
   "source": [
    "# # including necessary files and latex fonts\n",
    "# !cp drive/MyDrive/DNN/HubbardNet_gpu.py .\n",
    "# !cp drive/MyDrive/DNN/matrix_element.py .\n",
    "# !sudo apt install cm-super dvipng texlive-latex-extra texlive-latex-recommended\n",
    "# !pip install matplotlib==3.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Lfug1k7VbgUv"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dw/00xfphjj7mqfdx9qd2n3glth0000gp/T/ipykernel_32994/1424705224.py:23: MatplotlibDeprecationWarning: Support for setting an rcParam that expects a str value to a non-str value is deprecated since 3.5 and support will be removed two minor releases later.\n",
      "  matplotlib.rcParams['text.latex.preamble']=[r'\\usepackage{amsmath}']\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from matplotlib import rc\n",
    "import numpy as np\n",
    "\n",
    "from HubbardNet_gpu import *\n",
    "import matrix_element as me\n",
    "\n",
    "from os import path\n",
    "import os\n",
    "from copy import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "plt.rc('text',usetex=True)\n",
    "#font = {'family':'serif','size':16}\n",
    "font = {'family':'serif','size':25, 'serif': ['computer modern roman']}\n",
    "plt.rc('font',**font)\n",
    "matplotlib.rcParams['text.latex.preamble']=[r'\\usepackage{amsmath}']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GKDQu5TPcR-p",
    "outputId": "02222d89-b3ec-404e-ae92-fba3a23d919e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU.\n"
     ]
    }
   ],
   "source": [
    "# Go to Edit -> Notebook Settings and select \"GPU\" from the hardware accelerator dropdown. \n",
    "# If this is on, GPU is enabled by default\n",
    "\n",
    "use_gpu = False\n",
    "\n",
    "# Check to see if gpu is available. If it is, use it else use the cpu\n",
    "if torch.cuda.is_available() and use_gpu:\n",
    "    device = torch.device('cuda')\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "    print('Using GPU.')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    torch.set_default_tensor_type('torch.DoubleTensor')\n",
    "\n",
    "    if not torch.cuda.is_available() and use_gpu: \n",
    "        use_gpu = False \n",
    "        print('GPU not available. Using CPU.')\n",
    "    else: \n",
    "        print('Using CPU.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # charge gap & chemical potential (ED)\n",
    "# M_list = np.arange(3, 6)\n",
    "# U_list = np.linspace(2, 100, 20)\n",
    "# t = 1\n",
    "# V = 0\n",
    "# mu = 0\n",
    "# pbc = True\n",
    "\n",
    "# E_gs = np.zeros( (len(N_list), len(M_list), len(U_list)) )\n",
    "# for (m_idx, M) in enumerate(M_list):\n",
    "    \n",
    "#     N_list = [M-1, M, M+1]\n",
    "#     model_list = []\n",
    "#     H_list = []\n",
    "\n",
    "#     w = M\n",
    "#     h = int(M/w)\n",
    "    \n",
    "\n",
    "#     for (n_idx, N_here) in enumerate(N_list): \n",
    "#         O = N_here # maximum number of particles \n",
    "#         model = me.Bose_Hubbard(N_here, O, w=w, h=h, M=M, pbc=pbc)\n",
    "#         for (U_idx, U) in enumerate(U_list):\n",
    "#             _,_,H = model.H_Bose_Hubbard(t, U, V=V, mu=mu)\n",
    "#             vals, vecs = np.linalg.eig( H )\n",
    "            \n",
    "#             E_gs[n_idx, m_idx, U_idx] = np.min(vals.real)\n",
    "            \n",
    "#     print(\"Done with lattice size {}/{}\".format(m_idx+1, len(M_list)))\n",
    "\n",
    "\n",
    "# # charge gap \n",
    "# cgap = E_gs[0] + E_gs[2] - 2*E_gs[1]\n",
    "# mu_plus = E_gs[2] - E_gs[1]\n",
    "# mu_minus = E_gs[1] - E_gs[0]\n",
    "\n",
    "    \n",
    "# # print(\"The size of the Hamiltonian is {}\".format(model.tot_states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig,ax=plt.subplots(figsize=(7,5))\n",
    "# for (m_idx, M) in enumerate(M_list):\n",
    "#     color = next(ax._get_lines.prop_cycler)['color']\n",
    "#     ax.plot(t/U_list, mu_plus[m_idx]/U_list, '-', color=color, label='$M = ' +str(M) +\"$\")\n",
    "#     ax.plot(t/U_list, mu_minus[m_idx]/U_list, '-', color=color)\n",
    "#     ax.set_xlabel('$t/U$')\n",
    "#     ax.set_ylabel('$\\mu^\\pm/U$')\n",
    "#     ax.legend(fontsize=15)\n",
    "#     ax.set_ylim([-0.23, 1])\n",
    "#     ax.set_xlim([0, 0.5])\n",
    "# plt.show()\n",
    "# # plt.savefig('./figures/ED_chemical_potential.pdf', format='pdf',bbox_inches='tight')\n",
    "\n",
    "# # plt.figure()\n",
    "# # plt.plot(t/U_list,cgap/U_list, '-s')\n",
    "# # plt.xlabel('$t/U$')\n",
    "# # plt.ylabel('Charge gap $(t)$')\n",
    "# # plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the Hamiltonian is 924\n",
      "The size of the Hamiltonian is 1716\n",
      "The size of the Hamiltonian is 3003\n"
     ]
    }
   ],
   "source": [
    "model_list = []\n",
    "\n",
    "M = 7\n",
    "N_list = [M-1,M,M+1]\n",
    "\n",
    "w = M\n",
    "h = int(M/w)\n",
    "pbc = True\n",
    "\n",
    "for (n_idx, N) in enumerate(N_list): \n",
    "    O = N\n",
    "    \n",
    "    model = me.Bose_Hubbard(N, O, w=w, h=h, M=M, pbc=pbc)\n",
    "    model_list.append(model)\n",
    "    \n",
    "    print(\"The size of the Hamiltonian is {}\".format(model.tot_states))\n",
    "\n",
    "t = 1 \n",
    "U_max = 5\n",
    "V = 0\n",
    "U_list_all = np.arange(1, U_max+1, .5)\n",
    "mu_list_all = np.linspace(0, 10, 5)\n",
    "mu_list_all = np.array([0.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "560604jwhqey"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## NN\n",
    "\n",
    "S = 50 # number of sample of the MH sampler (not used)\n",
    "init = 1 # the first state to sample in Metropolis Hastings (has nothing to do with the optimizer!) (not used)\n",
    "\n",
    "# Network parameters\n",
    "D_hid = 400 # the number ofneurons in the hidden layer\n",
    "lr = 0.01 # learning rate \n",
    "epochs = 70000\n",
    "loss_diff = 1e-7\n",
    "grad_cut = 1e-6\n",
    "check_point = 100 # print out the energy every X points\n",
    "use_sampler = False # for now, only support ground state (not working anyway)\n",
    "\n",
    "# Model parameters\n",
    "U_train = np.ones(3)*2\n",
    "t_train = 1.\n",
    "\n",
    "U_train = np.array([4., 7, 10])\n",
    "mu_train = np.zeros_like(U_train)\n",
    "\n",
    "min_state = 0\n",
    "max_state = 1\n",
    "\n",
    "n_excited = max_state - 1\n",
    "\n",
    "# paths to save and load weights \n",
    "fpath = os.getcwd()+'/weights/'\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "loss_all = [] \n",
    "penalty_all = []\n",
    "nn = []\n",
    "\n",
    "# filepath for excited states\n",
    "fname = fpath + \"/weights_multi_N_M{}_Umax{}_Umin{}\".format(M,np.max(U_train),np.min(U_train),D_hid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "97XH_A3ol_l_",
    "outputId": "df75b1ac-613f-4ae8-8318-744e3606c5e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin optimizing for state 0\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zoe/Desktop/CMT/DNN/HubbardNet/HubbardNet_gpu.py:727: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at  ../aten/src/ATen/native/Copy.cpp:240.)\n",
      "  H_list.append(torch.tensor(H, dtype=torch.double))\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:508: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, current learning rate 0.01, loss 24.47518865483529, total gradient 4.9697569600992715\n",
      "Iteration 100, current learning rate 0.00975045441899919, loss 20.601238279474092, total gradient 3.2018436959501964\n",
      "Iteration 200, current learning rate 0.009035881413933598, loss 24.51689265087994, total gradient -0.028023894694732855\n",
      "Iteration 300, current learning rate 0.007926253039256266, loss 24.51231259504044, total gradient -0.005693154558566148\n",
      "Iteration 400, current learning rate 0.0065301874465082295, loss 21.46714605036063, total gradient 2.945790288948739\n",
      "Iteration 500, current learning rate 0.004984341255885152, loss 21.2888118534903, total gradient 54.903933312726295\n",
      "Iteration 600, current learning rate 0.0034400326517927448, loss 21.535874240762524, total gradient -12.146865200202802\n",
      "Iteration 700, current learning rate 0.002048429298162417, loss 22.111266118865753, total gradient -0.6007079041860954\n",
      "Iteration 800, current learning rate 0.0009457509696090281, loss 22.04297854677185, total gradient -0.060600245323464225\n",
      "Iteration 900, current learning rate 0.00023993524647491373, loss 22.035563392390593, total gradient -0.049315645860331825\n",
      "Reset scheduler\n",
      "Iteration 1000, current learning rate 0.01, loss 22.034869308444563, total gradient -0.04885015417538179\n",
      "Iteration 1100, current learning rate 0.00975045441899919, loss 24.514026152428663, total gradient -0.023587206408173095\n",
      "Iteration 1200, current learning rate 0.009035881413933598, loss 23.64718924478457, total gradient -3.071987835373623\n",
      "Iteration 1300, current learning rate 0.007926253039256266, loss 23.137761098519793, total gradient 6.390822213515373\n",
      "Iteration 1400, current learning rate 0.0065301874465082295, loss 24.513590606042516, total gradient -0.007615787516193335\n",
      "Iteration 1500, current learning rate 0.004984341255885152, loss 24.519076683110818, total gradient 0.004015792611263428\n",
      "Iteration 1600, current learning rate 0.0034400326517927448, loss 22.447979234775037, total gradient 2.002422406125764\n",
      "Iteration 1700, current learning rate 0.002048429298162417, loss 23.665281889768412, total gradient 2.409243485680633\n",
      "Iteration 1800, current learning rate 0.0009457509696090281, loss 17.29748891537074, total gradient 2.523295608832602\n",
      "Iteration 1900, current learning rate 0.00023993524647491373, loss 17.141137117491382, total gradient 0.27186274261454446\n",
      "Reset scheduler\n",
      "Iteration 2000, current learning rate 0.01, loss 17.138553441252377, total gradient 0.28115382444880044\n",
      "Iteration 2100, current learning rate 0.00975045441899919, loss 20.28639333868946, total gradient 31.036362764188308\n",
      "Iteration 2200, current learning rate 0.009035881413933598, loss 17.48352623972447, total gradient 0.24265049289540175\n",
      "Iteration 2300, current learning rate 0.007926253039256266, loss 20.893366143666682, total gradient 48.018994476289876\n",
      "Iteration 2400, current learning rate 0.0065301874465082295, loss 16.883415054572023, total gradient -6.215483973427191\n",
      "Iteration 2500, current learning rate 0.004984341255885152, loss 24.51122083268597, total gradient -0.11127788378956918\n",
      "Iteration 2600, current learning rate 0.0034400326517927448, loss 23.99877502554833, total gradient 25.733476223036618\n",
      "Iteration 2700, current learning rate 0.002048429298162417, loss 24.512509022159858, total gradient -0.04729338873950767\n",
      "Iteration 2800, current learning rate 0.0009457509696090281, loss 14.196109521918798, total gradient 22.042453133902516\n",
      "Iteration 2900, current learning rate 0.00023993524647491373, loss 11.040182028130197, total gradient 0.7972364130053782\n",
      "Reset scheduler\n",
      "Iteration 3000, current learning rate 0.01, loss 11.000899522603849, total gradient 0.7928104174334016\n",
      "Iteration 3100, current learning rate 0.00975045441899919, loss 10.99379602165957, total gradient -6.762730915588834\n",
      "Iteration 3200, current learning rate 0.009035881413933598, loss 24.513940835344115, total gradient -0.022467857863384823\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 50>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fc1, Loss_history, dot_history\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n_excited \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(min_state, max_state):\n\u001b[0;32m---> 52\u001b[0m     fc1, Loss_history, dot_history \u001b[38;5;241m=\u001b[39m \u001b[43mcall_NN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_excited\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(Loss_history))\n\u001b[1;32m     55\u001b[0m     nn\u001b[38;5;241m.\u001b[39mappend(fc1)\n",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36mcall_NN\u001b[0;34m(lr, n_excited)\u001b[0m\n\u001b[1;32m     17\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD_hid\u001b[39m\u001b[38;5;124m'\u001b[39m: D_hid, \n\u001b[1;32m     18\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep_size\u001b[39m\u001b[38;5;124m'\u001b[39m: lr, \n\u001b[1;32m     19\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_iteration\u001b[39m\u001b[38;5;124m'\u001b[39m:epochs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperturb_amp\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.0\u001b[39m, \n\u001b[1;32m     38\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdropout\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.0\u001b[39m}\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBegin optimizing for state \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_excited))\n\u001b[0;32m---> 42\u001b[0m fc1, Loss_history, dot_history, all_E_list \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_NN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmu_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mU_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mV\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43muse_sampler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_sampler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloadweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mfname_load\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_excited\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_excited\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fc1, Loss_history, dot_history\n",
      "File \u001b[0;32m~/Desktop/CMT/DNN/HubbardNet/HubbardNet_gpu.py:745\u001b[0m, in \u001b[0;36mtrain_NN\u001b[0;34m(model_list, N_list, mu_list, U_list, t, V, S, params, fname, use_sampler, init, loadweights, fname_load, n_excited)\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[38;5;66;03m# U_perturb = U_list\u001b[39;00m\n\u001b[1;32m    744\u001b[0m all_states, UtN_tensor \u001b[38;5;241m=\u001b[39m NN_inputs(model_list, N_list, U_perturb, mu_list, n_states\u001b[38;5;241m-\u001b[39mload_states)\n\u001b[0;32m--> 745\u001b[0m u \u001b[38;5;241m=\u001b[39m \u001b[43mfc0\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdouble\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mUtN_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdouble\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;66;03m# real part \u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gs_opt: \n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/CMT/DNN/HubbardNet/HubbardNet_gpu.py:368\u001b[0m, in \u001b[0;36mfeedforward.forward\u001b[0;34m(self, states, UtN_list)\u001b[0m\n\u001b[1;32m    366\u001b[0m l \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLin_1(torch\u001b[38;5;241m.\u001b[39mcat((states,UtN_list),\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    367\u001b[0m l \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLin_2(l);    h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactF(l)\n\u001b[0;32m--> 368\u001b[0m l \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLin_3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m;    h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactF(l)\n\u001b[1;32m    369\u001b[0m l \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLin_4(h);    h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactF(l)\n\u001b[1;32m    370\u001b[0m l \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLin_5(h);    h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactF(l)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/linear.py:96\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/functional.py:1847\u001b[0m, in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight):\n\u001b[1;32m   1846\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(linear, (\u001b[38;5;28minput\u001b[39m, weight), \u001b[38;5;28minput\u001b[39m, weight, bias\u001b[38;5;241m=\u001b[39mbias)\n\u001b[0;32m-> 1847\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def call_NN(lr, n_excited):\n",
    "    if n_excited: \n",
    "        if n_excited == 1 :\n",
    "            load_states_indv = [0]\n",
    "        else:\n",
    "            load_states_indv = range(1, n_excited)\n",
    "        gs_flag = False\n",
    "        es_flag = True\n",
    "\n",
    "    else: # ground states\n",
    "        load_states_indv = [0]\n",
    "        gs_flag = True\n",
    "        es_flag = False\n",
    "    \n",
    "    load_states = np.max(load_states_indv)  # total number of states being fixed \n",
    "  \n",
    "    params = {'D_hid': D_hid, \n",
    "              'step_size': lr, \n",
    "              'max_iteration':epochs,\n",
    "              'check_point': check_point,\n",
    "              'loss_diff': loss_diff, \n",
    "              'steps': 1, # reset learning every N steps\n",
    "              'loss_check_steps': 50, # check the local every N steps\n",
    "              'grad_cut': grad_cut,  # stopping condition in the total gradient \n",
    "              'weight_init': False, \n",
    "              'zero_bias': False, \n",
    "              'gs_epochs': 1000, # the maximum number of steps to minimize the ground state\n",
    "              'gs_flag': gs_flag, # ground state only\n",
    "              'es_flag': es_flag,  # excited state only\n",
    "              'regularization': True, \n",
    "              'load_states': load_states, # the number of states loaded \n",
    "              'load_states_indv': load_states_indv, \n",
    "              'rand_steps': 5000, \n",
    "              'load_weights_from_previous_state': False, # randomize the projection every N steps\n",
    "              'use_gpu': use_gpu, \n",
    "              'weight_decay': 0.01,\n",
    "              'perturb_amp': 0.0, \n",
    "              'dropout': 0.0}\n",
    "  \n",
    "    print(\"Begin optimizing for state {}\".format(n_excited))\n",
    "\n",
    "    fc1, Loss_history, dot_history, all_E_list = train_NN(model_list, N_list, mu_train, U_train, t_train, V, S, params, fname, \\\n",
    "                                            use_sampler=use_sampler, init=init, loadweights=False,\\\n",
    "                                            fname_load=fname, n_excited=n_excited)\n",
    "\n",
    "    return fc1, Loss_history, dot_history\n",
    "\n",
    "\n",
    "\n",
    "for n_excited in range(min_state, max_state):\n",
    "    \n",
    "    fc1, Loss_history, dot_history = call_NN(lr, n_excited)\n",
    "    print(len(Loss_history))\n",
    "\n",
    "    nn.append(fc1)\n",
    "    loss_all.append(Loss_history)\n",
    "    penalty_all.append(dot_history)\n",
    "    \n",
    "tf = time.time()\n",
    "print(\"Training time = {} seconds.\".format(tf-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 358
    },
    "id": "mi1QNIyMmee4",
    "outputId": "12983a86-a2a7-4a21-931a-442ee39889c6"
   },
   "outputs": [],
   "source": [
    "\n",
    "for i in range(1): \n",
    "    Loss_history = loss_all[i]\n",
    "    Loss_history = np.array(Loss_history)\n",
    "    \n",
    "    fig,ax=plt.subplots(figsize=(7,5))\n",
    "    ax.plot(Loss_history - np.min(Loss_history), '-', label='loss')\n",
    "    ax.set_xlabel(\"Iteration\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    # ax.set_title(\"U/t = {}\".format(U_train))\n",
    "    ax.set_yscale('log')\n",
    "    \n",
    "    \n",
    "    # ax.plot(dot_history, '-', label='regularization')\n",
    "    ax.set_xlabel(\"Iteration\")\n",
    "    # ax.set_ylabel(\"Dot product\")\n",
    "    # ax.set_title(\"U/t = {}\".format(U_train))\n",
    "    # ax.legend()\n",
    "    plt.savefig('./figures/loss_M{}.pdf'.format(M,N,range(min_state, max_state)[i]), format='pdf',bbox_inches='tight')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JNSHzI8XmgID",
    "outputId": "9e1de8c0-9113-447d-a910-1979828d8280"
   },
   "outputs": [],
   "source": [
    "E_all = np.zeros(( len(U_train), model.tot_states) )\n",
    "# check solution \n",
    "for (i, U) in enumerate(U_train):\n",
    "    _, _, H = model.H_Bose_Hubbard(t, U, V=V, mu=mu_train[i])\n",
    "    evals, evecs = np.linalg.eig(H)\n",
    "    idx = np.argsort(evals)\n",
    "    evecs = evecs[:,idx]\n",
    "    evals = evals[idx]\n",
    "    E_all[i] = evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4WZpMkuW0ja0"
   },
   "outputs": [],
   "source": [
    "U_test = np.linspace(1.5, 15, 20)\n",
    "mu_test = np.zeros_like(U_test)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7,5))\n",
    "\n",
    "colors = ['r','b', 'k']\n",
    "for j,model in enumerate(model_list):\n",
    "    E_train, wf_gs = wf_e_calc(model_list[j], N_list[j], U_train, mu_train, t, V, 0, 0, fc1, use_gpu=True)\n",
    "    E_test, wf_gs_test = wf_e_calc(model_list[j], N_list[j], U_test, mu_test, t, V, 0, 0, fc1, use_gpu=True)\n",
    "\n",
    "    \n",
    "    ax.scatter(U_train,E_train,s=50,c=colors[j], marker='s')\n",
    "    ax.scatter(U_test,E_test,s=50, c=colors[j], marker='x',lw=2)\n",
    "\n",
    "    arr = [] \n",
    "    for (i, U) in enumerate(U_test):\n",
    "        _, _, H = model.H_Bose_Hubbard(t, U, V=V, mu=mu_train[0])\n",
    "        vals, vecs = np.linalg.eig(H)\n",
    "        vals_idx = np.argsort(vals)\n",
    "        vals = np.sort(vals)\n",
    "        arr.append(vals[0])\n",
    "    ax.plot(U_test,arr,colors[j],label='N={}'.format(N_list[j]))\n",
    "ax.set_xlim([min(U_test), max(U_test)])  \n",
    "ax.set_ylabel('Energy')\n",
    "ax.set_xlabel('$U$')\n",
    "plt.legend(frameon=False,prop={'size': 16})\n",
    "plt.savefig(\"./figures/energy_M{}.pdf\".format(M), format='pdf',bbox_inches='tight')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "id": "szreTRyvwcZX",
    "outputId": "e291ca65-c99c-46fa-d735-99678c71cdaa"
   },
   "outputs": [],
   "source": [
    "# chemical potential & charge gap\n",
    "U_test = np.linspace(0.1, 20, 40)\n",
    "mu_test = np.zeros_like(U_test)\n",
    "E_gs_nn = np.zeros( (len(N_list), len(U_test)) )\n",
    "E_gs_train = np.zeros( (len(N_list), len(U_train)) )\n",
    "\n",
    "colors = ['r','b','k']\n",
    "for j,model in enumerate(model_list):\n",
    "    E_test, wf_gs_test = wf_e_calc(model_list[j], N_list[j], U_test, mu_test, t, V, 0, 0, fc1, use_gpu=True)\n",
    "    E_train, wf_gs = wf_e_calc(model_list[j], N_list[j], U_train, mu_train, t, V, 0, 0, fc1, use_gpu=True)\n",
    "    E_gs_nn[j,:] = E_test.T\n",
    "    E_gs_train[j,:] = E_train.T\n",
    "    \n",
    "cgap = E_gs_nn[0] + E_gs_nn[2] - 2*E_gs_nn[1]\n",
    "mu_plus = E_gs_nn[2] - E_gs_nn[1]\n",
    "mu_minus = E_gs_nn[1] - E_gs_nn[0]\n",
    "\n",
    "cgap_train = E_gs_train[0] + E_gs_train[2] - 2*E_gs_train[1]\n",
    "mu_plus_train = E_gs_train[2] - E_gs_train[1]\n",
    "mu_minus_train = E_gs_train[1] - E_gs_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "4USQHKkMjGVP",
    "outputId": "e1bfe25a-00f4-4529-8564-c4d81dea11dd"
   },
   "outputs": [],
   "source": [
    "fig, ax=plt.subplots(figsize=(7,5))\n",
    "color = next(ax._get_lines.prop_cycler)['color']\n",
    "ax.plot(t/U_test, mu_plus/U_test, 'x', color=color)\n",
    "ax.plot(t/U_test, mu_minus/U_test, 'x', color=color)\n",
    "ax.plot(t/U_train, mu_plus_train/U_train, 's', color=color)\n",
    "ax.plot(t/U_train, mu_minus_train/U_train, 's', color=color)\n",
    "\n",
    "ax.set_xlabel('$t/U$')\n",
    "ax.set_ylabel('$\\mu^\\pm/U$')\n",
    "ax.set_ylim([-0.23, 1])\n",
    "ax.set_xlim([0, 0.5])\n",
    "plt.show()\n",
    "plt.savefig('./figures/NN_chemical_potential_M{}.pdf'.format(M), format='pdf',bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "id": "DY5M3VF7s5fV",
    "outputId": "34d2142a-8753-4919-f1ce-7ee675604849"
   },
   "outputs": [],
   "source": [
    "n_list = np.zeros( (len(model_list),M) )\n",
    "\n",
    "fig, ax = plt.subplots(1, len(model_list), figsize=(15, 5))\n",
    "U_test = np.array( [5] ) \n",
    "\n",
    "for model_idx,model in enumerate(model_list):\n",
    "    # rearrange states by symmetry \n",
    "    all_states = np.zeros_like(model.all_states)\n",
    "    idx_list = np.zeros(model.tot_states,dtype=int)\n",
    "\n",
    "    for i in range(int(model.tot_states/2)): \n",
    "        all_states[i] = model.all_states[i]\n",
    "        all_states[-i-1] = np.flip(model.all_states[i])\n",
    "        idx_list[i] = i\n",
    "        for j in range(model.tot_states):\n",
    "            if all(model.all_states[j]==all_states[-i-1]): \n",
    "                idx_list[-i-1] = j\n",
    "                break\n",
    "\n",
    "    # check wavefunctions \n",
    "    \n",
    "    mu_test = np.zeros(len(U_test))\n",
    "    vals_all = np.zeros((model.tot_states, len(U_test)))\n",
    "    vals_idx_all = np.zeros_like(vals_all)\n",
    "    vecs_all = np.zeros((model.tot_states, model.tot_states, len(U_test)))\n",
    "\n",
    "    \n",
    "    _, _, H = model.H_Bose_Hubbard(t, U, mu=mu_train[0])\n",
    "    vals, vecs = np.linalg.eig(H)\n",
    "    vals_idx_all[:,0] = np.argsort(vals)\n",
    "    vals_all[:,0] = vals\n",
    "    vecs_all[:,:,0] = vecs\n",
    "\n",
    "    E_test, wf_test = wf_e_calc(model, N_list[model_idx], U_test, mu_test, t, V, 0, 0, fc1, use_gpu=True)\n",
    "    \n",
    "    s = 0 \n",
    "    if model_idx == 0:\n",
    "        ymax = 0.4\n",
    "    else: \n",
    "        ymax = 0.3\n",
    "    ymin = 0\n",
    "  \n",
    "    i=0\n",
    "    wf_here = wf_test[i*model.tot_states:(i+1)*model.tot_states,s].squeeze().double()\n",
    "\n",
    "    # calculate occupation numbers\n",
    "    for m_idx in range(M):\n",
    "        for state_idx in range(model.tot_states):\n",
    "            ci = wf_here[state_idx].cpu().detach().numpy()\n",
    "            ni = model.all_states[state_idx][m_idx]\n",
    "            n_list[model_idx,m_idx] += ci**2 * ni\n",
    "\n",
    "    wf_exact = np.abs(vecs_all[:,int(vals_idx_all[s, i]), i]).squeeze()\n",
    "    wf_nn = np.abs(wf_here.cpu().detach().numpy())\n",
    "    ax[model_idx].plot(wf_exact[idx_list], label=\"ED\")\n",
    "    ax[model_idx].plot(wf_nn[idx_list], 'x--', label=\"HubbardNet\")\n",
    "    \n",
    "    ax[model_idx].set_title(\"N = {}\".format(N_list[model_idx]))\n",
    "    ax[model_idx].set_ylim((ymin,ymax))\n",
    "\n",
    "ax[0].set_ylabel('$|\\Psi_0 (\\mathbf{n})|$')\n",
    "ax[0].set_xlabel('Component')\n",
    "ax[1].set_xlabel('Component')\n",
    "ax[1].legend(frameon=True, prop={'size': 16})\n",
    "plt.savefig(os.getcwd() + \"/figures/wf_multi_N_M{}_gs.pdf\".format(M,N_list[model_idx],s,n_excited), format='pdf',bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gUVsCG8OKx0X"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(wf_nn[idx_list])\n",
    "# plt.plot(wf_exact[idx_list])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(wf_exact[idx_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f1XVgxi7KyUv"
   },
   "outputs": [],
   "source": [
    "np.sum(wf_exact[idx_list]**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_list_srt = np.sort(idx_list)\n",
    "plt.figure()\n",
    "plt.plot(idx_list_srt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
